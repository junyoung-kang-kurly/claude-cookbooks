{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 평가자-최적화기 워크플로우\n이 워크플로우에서는 하나의 LLM 호출이 응답을 생성하고, 다른 하나가 루프에서 평가와 피드백을 제공합니다.\n\n### 이 워크플로우를 사용해야 하는 경우\n이 워크플로우는 다음과 같은 경우에 특히 효과적입니다:\n\n- 명확한 평가 기준이 있을 때\n- 반복적인 개선에서 가치를 얻을 수 있을 때\n\n적합한 두 가지 신호는 다음과 같습니다:\n\n- 피드백이 제공되면 LLM 응답이 명백하게 개선될 수 있음\n- LLM이 의미 있는 피드백을 스스로 제공할 수 있음"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from util import llm_call, extract_xml\n\n\ndef generate(prompt: str, task: str, context: str = \"\") -> tuple[str, str]:\n    \"\"\"피드백을 기반으로 솔루션을 생성하고 개선합니다.\"\"\"\n    full_prompt = f\"{prompt}\\n{context}\\nTask: {task}\" if context else f\"{prompt}\\nTask: {task}\"\n    response = llm_call(full_prompt)\n    thoughts = extract_xml(response, \"thoughts\")\n    result = extract_xml(response, \"response\")\n\n    print(\"\\n=== 생성 시작 ===\")\n    print(f\"생각:\\n{thoughts}\\n\")\n    print(f\"생성됨:\\n{result}\")\n    print(\"=== 생성 종료 ===\\n\")\n\n    return thoughts, result\n\n\ndef evaluate(prompt: str, content: str, task: str) -> tuple[str, str]:\n    \"\"\"솔루션이 요구사항을 충족하는지 평가합니다.\"\"\"\n    full_prompt = f\"{prompt}\\n원래 작업: {task}\\n평가할 내용: {content}\"\n    response = llm_call(full_prompt)\n    evaluation = extract_xml(response, \"evaluation\")\n    feedback = extract_xml(response, \"feedback\")\n\n    print(\"=== 평가 시작 ===\")\n    print(f\"상태: {evaluation}\")\n    print(f\"피드백: {feedback}\")\n    print(\"=== 평가 종료 ===\\n\")\n\n    return evaluation, feedback\n\n\ndef loop(task: str, evaluator_prompt: str, generator_prompt: str) -> tuple[str, list[dict]]:\n    \"\"\"요구사항이 충족될 때까지 생성과 평가를 반복합니다.\"\"\"\n    memory = []\n    chain_of_thought = []\n\n    thoughts, result = generate(generator_prompt, task)\n    memory.append(result)\n    chain_of_thought.append({\"thoughts\": thoughts, \"result\": result})\n\n    while True:\n        evaluation, feedback = evaluate(evaluator_prompt, result, task)\n        if evaluation == \"PASS\":\n            return result, chain_of_thought\n\n        context = \"\\n\".join(\n            [\"이전 시도:\", *[f\"- {m}\" for m in memory], f\"\\n피드백: {feedback}\"]\n        )\n\n        thoughts, result = generate(generator_prompt, task, context)\n        memory.append(result)\n        chain_of_thought.append({\"thoughts\": thoughts, \"result\": result})"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 예제 사용 사례: 반복적인 코딩 루프\n\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "evaluator_prompt = \"\"\"\n다음 코드 구현을 평가하세요:\n1. 코드 정확성\n2. 시간 복잡도\n3. 스타일 및 모범 사례\n\n평가만 수행하고 작업을 해결하려고 시도하지 마세요.\n모든 기준이 충족되고 더 이상 개선 제안이 없을 때만 \"PASS\"를 출력하세요.\n다음 형식으로 평가를 간결하게 출력하세요.\n\n<evaluation>PASS, NEEDS_IMPROVEMENT, 또는 FAIL</evaluation>\n<feedback>\n무엇이 개선되어야 하는지와 그 이유.\n</feedback>\n\"\"\"\n\ngenerator_prompt = \"\"\"\n당신의 목표는 <사용자 입력>을 기반으로 작업을 완료하는 것입니다. 이전 생성에서 피드백이 있다면,\n솔루션을 개선하기 위해 이를 반영해야 합니다.\n\n다음 형식으로 답변을 간결하게 출력하세요: \n\n<thoughts>\n[작업과 피드백에 대한 이해, 그리고 개선 계획]\n</thoughts>\n\n<response>\n[여기에 코드 구현]\n</response>\n\"\"\"\n\ntask = \"\"\"\n<사용자 입력>\n다음 기능을 가진 Stack을 구현하세요:\n1. push(x)\n2. pop()\n3. getMin()\n모든 연산은 O(1)이어야 합니다.\n</사용자 입력>\n\"\"\"\n\nloop(task, evaluator_prompt, generator_prompt)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}